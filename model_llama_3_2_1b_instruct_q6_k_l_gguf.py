# -*- coding: utf-8 -*-
"""Aisha-Llama-3.2-1B-Instruct-Q6_K_L.gguf.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NfY0oMGH2gkn4JUmQ651uWhFwsVlhoFr
"""

import os
import pandas as pd
import numpy as np
import re
import ast
from huggingface_hub import login
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch

"""### Split CSV in Chunks"""

# import pandas as pd
# import os

# def split_csv(file_path, output_dir, chunk_size):
#     """
#     Splits a CSV file into smaller chunks.

#     Args:
#         file_path (str): Path to the input CSV file.
#         output_dir (str): Directory to save the chunk files.
#         chunk_size (int): Number of rows per chunk.
#     """
#     # Ensure output directory exists
#     os.makedirs(output_dir, exist_ok=True)

#     # Read and split the CSV into chunks
#     chunk_number = 1
#     for chunk in pd.read_csv(file_path, chunksize=chunk_size):
#         chunk_file = os.path.join(output_dir, f"chunk_{chunk_number}.csv")
#         chunk.to_csv(chunk_file, index=False)
#         print(f"Chunk {chunk_number} saved to {chunk_file}")
#         chunk_number += 1

# # Usage example
# file_path = "TestingData.csv"
# output_dir = "output_chunks"
# chunk_size = 100  # Number of rows per chunk

# split_csv(file_path, output_dir, chunk_size)

"""# Llama-3.2-1B-Instruct-Q6_K_L.gguf (Working Draft)


"""

pip install llama-cpp-python

import os
import pandas as pd
import numpy as np
import re
import ast
from huggingface_hub import login
import torch
from llama_cpp import Llama

# Authenticate Hugging Face Hub
login(LOGIN)

class LLamaPredictor():
    def __init__(self) -> None:
        # Model setup for llama_cpp Llama with custom parameters
        self.model = Llama.from_pretrained(
            repo_id="bartowski/Llama-3.2-1B-Instruct-GGUF",
            filename="Llama-3.2-1B-Instruct-Q6_K_L.gguf",
        )
        self.system_prompt = """You are an expert AI system that accurately answers financial questions. Each question will either be i) multiple choice, and ask you to choose the correct answer from a list of possible answers. Each possible answer will have a numerical label associated with it. Please choose the correct numerical label; OR ii) a free-response answer asking you to calculate a number - your output for this type of question should be a number representing the correct answer to the calculation the question asks for. For each question, you will be told if it is multiple choice or free-response. Please answer each question with total accuracy, performing all necessary calculations without skipping or simplifying any steps along the way.

You have years of expertise in the financial system and are absolutely the best at what you do. Your compensation is tied to your performance, and you stand to make millions of dollars if you answer all questions correctly."""

    def make_prediction(self, question_text: str, answer_options: list[str]) -> str:
        if  len(answer_options) == 0:
            # Free-response question format
            full_prompt = (
                f"{self.system_prompt}\n\n"
                f"This is a free-response question. Please answer this question with total accuracy. At the very end of your answer, please say the final correct number alone on a new line. Question: {question_text}"
            )
        else:
            # Multiple-choice question format
            full_prompt = (
                f"{self.system_prompt}\n\n"
                f"This is a multiple-choice question. Please answer this question with total accuracy. NOTE - YOU MUST DO THE FOLLOWING: At the very end of your output, please say the numerical label associated with the correct answer alone on a new line. Remember, this label should be the first part of the input associated with the correct answer [for instance, for (0, correct option) you would say '0'] and can ONLY be one of the numbers you saw associated with a question choice. Question: {question_text}. Possible answers: {answer_options}"
            )

        #       # Limit the character length to prevent exceeding the context limit
        # if len(full_prompt) > 512:
        #     full_prompt = full_prompt[:512]  # Truncate to the first 512 characters


        print("Generated Prompt:", full_prompt)  # Debug: print prompt for troubleshooting

        # Generate response
        response = self.model(full_prompt, max_tokens=1024, temperature=0.6, top_p=0.9)

        # Debugging: Directly print the raw response from the model
        print("Raw Model Response:", response)

        # Process and clean up the response text
        response_text = response.get("choices", [{}])[0].get("text", "").strip()
        print("Processed Model Response:", response_text)  # Debug: check cleaned response
        return response_text

    def make_predictions(self) -> None:
        df = pd.read_csv(f'chunk_3.csv')
        df = df.drop('Unnamed: 0', axis=1)
        df['options'] = df['options'].fillna(-1)
        df['context'] = df['context'].fillna(-1)
        outputs = []
        for input_id, question_text, answer_options, context in zip(df['id'].values, df['question'].values, df['options'].values, df['context'].values):
            if answer_options == -1:
                answer_options = []
                if context != -1:
                    question_text += f"; Here's the necessary context to answer that question: {context}"
            elif type(answer_options) == type(''):
                answer_options = ast.literal_eval(answer_options)
            i = 0
            new_options = []
            for x in answer_options:
                new_options.append((i, x))
                i+=1
            prediction = self.make_prediction(question_text, new_options)
            print('='*100)
            print('question text was:', question_text)
            print('options were:', new_options)
            print('prediction is:', prediction)
            print('\n')
            outputs.append({'id':input_id, 'response':prediction})
        return outputs

# def extract_final_answer(complete_answer_text: str):
#     complete_answer_text = re.sub(r'[,$]', '', complete_answer_text).strip()
#     last_number = re.findall(r'-?\d+\.?\d*', complete_answer_text)[-1]
#     last_number = float(last_number)
#     return int(last_number) if last_number.is_integer() else last_number

def extract_final_answer(complete_answer_text: str):
    complete_answer_text = re.sub(r'[,$]', '', complete_answer_text).strip()
    numbers = re.findall(r'-?\d+\.?\d*', complete_answer_text)
    if not numbers:
        # Handle the case where no number is found
        return None  # Or return a default value like `float('nan')`
    last_number = float(numbers[-1])
    return int(last_number) if last_number.is_integer() else last_number



def force_int(df):
    for idx, row in df.iterrows():
        if row['response'].is_integer():
            df.loc[idx, 'response'] = int(row['response'])
    return df

import os
import pandas as pd
import numpy as np
import re
import ast
from huggingface_hub import login
import torch
from llama_cpp import Llama

# Authenticate Hugging Face Hub
login(LOGIN)

class LLamaPredictor():
    def __init__(self) -> None:
        # Model setup for llama_cpp Llama with custom parameters
        self.model = Llama.from_pretrained(
            repo_id="bartowski/Llama-3.2-1B-Instruct-GGUF",
            filename="Llama-3.2-1B-Instruct-Q6_K_L.gguf",
        )
        self.system_prompt = """You are an expert AI system that accurately answers financial questions. Each question will either be i) multiple choice, and ask you to choose the correct answer from a list of possible answers. Each possible answer will have a numerical label associated with it. Please choose the correct numerical label; OR ii) a free-response answer asking you to calculate a number - your output for this type of question should be a number representing the correct answer to the calculation the question asks for. For each question, you will be told if it is multiple choice or free-response. Please answer each question with total accuracy, performing all necessary calculations without skipping or simplifying any steps along the way.

You have years of expertise in the financial system and are absolutely the best at what you do. Your compensation is tied to your performance, and you stand to make millions of dollars if you answer all questions correctly."""

    def make_prediction(self, question_text: str, answer_options: list[str]) -> str:
        if  len(answer_options) == 0:
            # Free-response question format
            full_prompt = (
                f"{self.system_prompt}\n\n"
                f"This is a free-response question. Please answer this question with total accuracy. At the very end of your answer, please say the final correct number alone on a new line. Question: {question_text}"
            )
        else:
            # Multiple-choice question format
            full_prompt = (
                f"{self.system_prompt}\n\n"
                f"This is a multiple-choice question. Please answer this question with total accuracy. NOTE - YOU MUST DO THE FOLLOWING: At the very end of your output, please say the numerical label associated with the correct answer alone on a new line. Remember, this label should be the first part of the input associated with the correct answer [for instance, for (0, correct option) you would say '0'] and can ONLY be one of the numbers you saw associated with a question choice. Question: {question_text}. Possible answers: {answer_options}"
            )

        #       # Limit the character length to prevent exceeding the context limit
        # if len(full_prompt) > 512:
        #     full_prompt = full_prompt[:512]  # Truncate to the first 512 characters


        print("Generated Prompt:", full_prompt)  # Debug: print prompt for troubleshooting

        # Generate response
        response = self.model(full_prompt, max_tokens=1024, temperature=0.6, top_p=0.9)

        # Debugging: Directly print the raw response from the model
        print("Raw Model Response:", response)

        # Process and clean up the response text
        response_text = response.get("choices", [{}])[0].get("text", "").strip()
        print("Processed Model Response:", response_text)  # Debug: check cleaned response
        return response_text

    def make_predictions(self) -> list[dict]:
      # Load the input data
      df = pd.read_csv('chunk_3.csv')
      df = df.drop('Unnamed: 0', axis=1)
      df['options'] = df['options'].fillna(-1)
      df['context'] = df['context'].fillna(-1)

      outputs = []
      for input_id, question_text, answer_options, context in zip(df['id'].values, df['question'].values, df['options'].values, df['context'].values):
          # Handle context and options formatting
          if answer_options == -1:
              answer_options = []
              if context != -1:
                  question_text += f"; Here's the necessary context to answer that question: {context}"
          elif isinstance(answer_options, str):
              answer_options = ast.literal_eval(answer_options)

          # Format options with numerical labels
          new_options = [(i, x) for i, x in enumerate(answer_options)]

          # Check if the prompt would exceed token limit
          full_prompt = (
              f"{self.system_prompt}\n\n"
              f"This is a {'free-response' if not new_options else 'multiple-choice'} question."
              f"Question: {question_text}. Possible answers: {new_options}"
          )
          tokenized_prompt = self.model.tokenize(full_prompt.encode("utf-8"), add_bos=True)

          if len(tokenized_prompt) > 512:
              print(f"Skipping question {input_id} due to token limit.")
              outputs.append({'id': input_id, 'response': 0})  # Assign default value
              continue

          # Make prediction for valid prompts
          try:
              prediction = self.make_prediction(question_text, new_options)
              print('=' * 100)
              print('Question text was:', question_text)
              print('Options were:', new_options)
              print('Prediction is:', prediction)
              print('\n')
          except Exception as e:
              print(f"Error processing question {input_id}: {e}")
              prediction = 0  # Fallback value in case of an error

          # Append the result
          outputs.append({'id': input_id, 'response': prediction})
      return outputs


# def extract_final_answer(complete_answer_text: str):
#     complete_answer_text = re.sub(r'[,$]', '', complete_answer_text).strip()
#     last_number = re.findall(r'-?\d+\.?\d*', complete_answer_text)[-1]
#     last_number = float(last_number)
#     return int(last_number) if last_number.is_integer() else last_number

def extract_final_answer(complete_answer_text: str):
    complete_answer_text = re.sub(r'[,$]', '', complete_answer_text).strip()
    numbers = re.findall(r'-?\d+\.?\d*', complete_answer_text)
    if not numbers:
        # Handle the case where no number is found
        return None  # Or return a default value like `float('nan')`
    last_number = float(numbers[-1])
    return int(last_number) if last_number.is_integer() else last_number



def force_int(df):
    for idx, row in df.iterrows():
        if row['response'].is_integer():
            df.loc[idx, 'response'] = int(row['response'])
    return df

if __name__ == "__main__":
    model = LLamaPredictor()
    outputs = model.make_predictions()
    df = pd.DataFrame(outputs)
    df['response'] = df['response'].apply(extract_final_answer)
    df['response'] = df['response'].astype('object')
    df = force_int(df)
    # df.to_csv('output.csv', index=False, header=False)

df.to_csv('output_chunk_2.csv', index=False, header=False)



"""# Revised Code to Evaluate Model with Subset of FinanceBench (Llama-3.2-1B-Instruct-Q6_K_L.gguf)"""

import pandas as pd

df = pd.read_json("hf://datasets/PatronusAI/financebench/financebench_merged.jsonl", lines=True)

df

filtered_df = df[df['question_type'] == 'metrics-generated']
filtered_df

filtered_df.shape

print(filtered_df.dtypes)

# filtered_df.to_csv(r"C:\Users\aisha\Downloads\filtered_data.csv", index=False)

pip install llama-cpp-python

import os
import pandas as pd
import numpy as np
import re
import ast
from huggingface_hub import login
import torch
from llama_cpp import Llama

# Authenticate Hugging Face Hub
login(LOGIN)

class LLamaPredictor():
    def __init__(self) -> None:
        # Model setup for llama_cpp Llama with custom parameters
        self.model = Llama.from_pretrained(
            repo_id="bartowski/Llama-3.2-1B-Instruct-GGUF",
            filename="Llama-3.2-1B-Instruct-Q6_K_L.gguf",
        )
        self.system_prompt = """You are an expert AI system that accurately answers financial questions. Each question will be a free-response question asking you to calculate a number. Your output for this type of question should be a number representing the correct answer to the calculation the question asks for. Please answer each question with total accuracy, performing all necessary calcualtions without skipping or simplying any steps along the way.

You have years of expertise in the financial system and are absolutely the best at what you do. Your compensation is tied to your performance, and you stand to make millions of dollars if you answer all questions correctly."""

    def make_prediction(self, question_text: str) -> str:
        # Free-response question format
        full_prompt = (
            f"{self.system_prompt}\n\n"
            f"This is a free-response question. Please answer this question with total accuracy. "
            f"At the very end of your answer, please say the final correct number alone on a new line. "
            f"Question: {question_text}"
        )

        print("Generated Prompt:", full_prompt)  # Debug: print prompt for troubleshooting

        # Generate response
        response = self.model(full_prompt, max_tokens=1024, temperature=0.6, top_p=0.9)

        # Debugging: Directly print the raw response from the model
        print("Raw Model Response:", response)

        # Process and clean up the response text
        response_text = response.get("choices", [{}])[0].get("text", "").strip()
        print("Processed Model Response:", response_text)  # Debug: check cleaned response
        return response_text

    def make_predictions(self, df: pd.DataFrame) -> list[dict]:
        outputs = []
        for _, row in df.iterrows():
            question_text = row['question']

            # Make prediction for valid prompts
            try:
                prediction = self.make_prediction(question_text)
                print('=' * 100)
                print('Question text was:', question_text)
                print('Prediction is:', prediction)
                print('\n')
            except Exception as e:
                print(f"Error processing question {row['financebench_id']}: {e}")
                prediction = None  # Fallback value in case of an error

            # Append the result
            outputs.append({'financebench_id': row['financebench_id'], 'response': prediction})
        return outputs


def extract_final_answer(complete_answer_text: str):
    """Extract the final number from the model's output."""
    complete_answer_text = re.sub(r'[,$]', '', complete_answer_text).strip()
    numbers = re.findall(r'-?\d+\.?\d*', complete_answer_text)
    if not numbers:
        return None  # Handle the case where no number is found
    last_number = float(numbers[-1])
    return int(last_number) if last_number.is_integer() else last_number


def force_int(df):
    """Force integer conversion for numeric responses."""
    for idx, row in df.iterrows():
        if isinstance(row['response'], (float, int)) and float(row['response']).is_integer():
            df.at[idx, 'response'] = int(row['response'])
    return df

if __name__ == "__main__":
    df = filtered_df
    model = LLamaPredictor()
    outputs = model.make_predictions(filtered_df)
    results_df = pd.DataFrame(outputs)
    results_df['response'] = results_df['response'].apply(extract_final_answer)
    results_df = force_int(results_df)
    results_df.to_csv("predictions.csv", index=False)

results_df

import pandas as pd
import re

# Example: Cleaning the 'answer' column in filtered_df
def clean_answer_column(df):
    # Remove symbols like $ or % and keep only numerical values
    df['answer'] = df['answer'].apply(lambda x: re.sub(r'[^\d\.\-]', '', str(x)))
    # Convert the cleaned values to numeric, handling errors
    df['answer'] = pd.to_numeric(df['answer'], errors='coerce')
    return df

# Apply the cleaning function to filtered_df
new_df = clean_answer_column(filtered_df)

# Check the cleaned DataFrame
print(filtered_df[['answer']].head())

filtered_df

# Rename the 'answer' column in filtered_df to 'actual_answer'
filtered_df = filtered_df.rename(columns={'answer': 'actual_answer'})

# Merge filtered_df['financebench_id', 'actual_answer'] with results_df on 'financebench_id'
results_df = results_df.merge(filtered_df[['financebench_id', 'actual_answer']], on='financebench_id', how='left')

results_df

results_df.to_csv("new_predictions.csv")



"""# llmware/bling-1b-0.1"""

import pandas as pd

df = pd.read_json("hf://datasets/PatronusAI/financebench/financebench_merged.jsonl", lines=True)

filtered_df = df[df['question_type'] == 'metrics-generated']
filtered_df

import os
import pandas as pd
import numpy as np
import re
import ast
from huggingface_hub import login
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer

# Authenticate Hugging Face Hub
login(LOGIN)

class BlingPredictor:
    def __init__(self, model_name: str = "llmware/bling-1b-0.1"):
        """
        Initialize the BlingPredictor with the model from Hugging Face and setup the tokenizer.
        """
        # Load the Bling model and tokenizer from HuggingFace
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        # System prompt instructing the model to calculate financial answers
        self.system_prompt = """You are an expert AI system that accurately answers financial questions. Each question will be a free-response question asking you to calculate a number. Your output for this type of question should be a number representing the correct answer to the calculation the question asks for. Please answer each question with total accuracy, performing all necessary calculations without skipping or simplifying any steps along the way.

You have years of expertise in the financial system and are absolutely the best at what you do. Your compensation is tied to your performance, and you stand to make millions of dollars if you answer all questions correctly."""

        # Set device to CUDA or CPU
        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
        else:
            self.device = torch.device("cpu")

        self.model.to(self.device)

    def make_prediction(self, question_text: str) -> str:
        """
        Make a prediction using the model by formatting the input as a free-response question.
        """
        # Construct the full prompt for the model
        full_prompt = (
            f"{self.system_prompt}\n\n"
            f"This is a free-response question. Please answer this question with total accuracy. "
            f"At the very end of your answer, please say the final correct number alone on a new line. "
            f"Question: {question_text}"
        )

        print("Generated Prompt:", full_prompt)  # Debug: print prompt for troubleshooting

        # Tokenize the input and generate output
        inputs = self.tokenizer(full_prompt, return_tensors="pt", padding=True, truncation=True)

        # Add attention mask to inputs for proper attention
        attention_mask = inputs["attention_mask"]

        # Ensure pad_token_id is set to eos_token_id (or any other valid token) for open-ended generation
        pad_token_id = self.tokenizer.eos_token_id

        # Generate the output from the model
        outputs = self.model.generate(
            inputs["input_ids"].to(self.device),
            attention_mask=attention_mask.to(self.device),
            max_length=1024,
            temperature=0.6,
            top_p=0.9,
            eos_token_id=pad_token_id  # Set eos_token_id as pad_token_id
        )

        # Decode the model's response
        response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Process the response text
        response_text = response.strip()
        print("Processed Model Response:", response_text)  # Debug: check cleaned response
        return response_text


    def make_predictions(self, df: pd.DataFrame) -> list[dict]:
        """
        Given a DataFrame of questions, make predictions for each.
        """
        outputs = []
        for _, row in df.iterrows():
            question_text = row['question']

            # Make prediction for valid prompts
            try:
                prediction = self.make_prediction(question_text)
                print('=' * 100)
                print('Question text was:', question_text)
                print('Prediction is:', prediction)
                print('\n')
            except Exception as e:
                print(f"Error processing question {row['financebench_id']}: {e}")
                prediction = None  # Fallback value in case of an error

            # Append the result
            outputs.append({'financebench_id': row['financebench_id'], 'response': prediction})
        return outputs


def extract_final_answer(complete_answer_text: str):
    """Extract the final number from the model's output."""
    complete_answer_text = re.sub(r'[,$]', '', complete_answer_text).strip()
    numbers = re.findall(r'-?\d+\.?\d*', complete_answer_text)
    if not numbers:
        return None  # Handle the case where no number is found
    last_number = float(numbers[-1])
    return int(last_number) if last_number.is_integer() else last_number


def force_int(df):
    """Force integer conversion for numeric responses."""
    for idx, row in df.iterrows():
        if isinstance(row['response'], (float, int)) and float(row['response']).is_integer():
            df.at[idx, 'response'] = int(row['response'])
    return df

if __name__ == "__main__":
    df = filtered_df
    model = BlingPredictor()
    outputs = model.make_predictions(filtered_df)
    results_df = pd.DataFrame(outputs)
    results_df['response'] = results_df['response'].apply(extract_final_answer)
    results_df = force_int(results_df)
    results_df.to_csv("predictions.csv", index=False)

results_df

import pandas as pd
import numpy as np
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch
import re

class FinancePredictor():
    def __init__(self, model_name: str):
        # Load the Llama model and tokenizer
        self.model = AutoModelForCausalLM.from_pretrained(model_name)
        self.tokenizer = AutoTokenizer.from_pretrained(model_name)

        if torch.backends.mps.is_available():
            self.device = torch.device("mps")
        elif torch.cuda.is_available():
            self.device = torch.device("cuda")
        else:
            self.device = torch.device("cpu")

        self.model = self.model.to(self.device)  # Ensure the model is loaded on the correct device

        self.system_prompt = """You are an expert AI system that accurately answers financial questions. Each question will be a free-response answer asking you calculate a number - your output for this type of question should be a number representing the correct answer to the calculation the question asks for. Please answer each question with total accuracy, performing all necessary calculations without skipping or simplifying any steps along the way.

You have years of expertise in the financial system and are absolutely the best at what you do. Your compensation is tied to your performance, and you stand to make millions of dollars if you answer all questions correctly."""

    def make_prediction(self, question_text: str) -> str:
        # Manually format the conversation input
        prompt = f"{self.system_prompt}\nUser: This is a free-response question. Please answer this question with total accuracy, performing all necessary calculations without skipping or simplifying any steps. At the very end of your answer, please say the final correct number alone on a new line. Question: {question_text}\nAI:"

        # Tokenize the prompt
        tokenized_prompt = self.tokenizer(prompt, return_tensors="pt").to(self.device)

        terminators = [
            self.tokenizer.eos_token_id,
            self.tokenizer.convert_tokens_to_ids("<|eot_id|>")
        ]

        # Generate the model's output
        outputs = self.model.generate(
            tokenized_prompt['input_ids'],
            attention_mask=tokenized_prompt['attention_mask'],
            eos_token_id=terminators,
            do_sample=True,
            temperature=0.6,
            top_p=0.9,
            max_length=2056
        )

        response = outputs[0][tokenized_prompt['input_ids'].shape[-1]:]
        decoded_response = self.tokenizer.decode(response, skip_special_tokens=True).strip()

        return decoded_response

    def make_predictions(self, df: pd.DataFrame) -> list[dict]:
        outputs = []
        for _, row in df.iterrows():
            question_text = row['question']

            # Make prediction for valid prompts
            try:
                prediction = self.make_prediction(question_text)
                print('=' * 100)
                print('Question text was:', question_text)
                print('Prediction is:', prediction)
                print('\n')
            except Exception as e:
                print(f"Error processing question {row['financebench_id']}: {e}")
                prediction = None  # Fallback value in case of an error

            # Append the result
            outputs.append({'financebench_id': row['financebench_id'], 'response': prediction})
        return outputs

    def extract_final_answer(self, complete_answer_text: str):
        """Extract the final number from the model's output."""
        complete_answer_text = re.sub(r'[,$]', '', complete_answer_text).strip()
        numbers = re.findall(r'-?\d+\.?\d*', complete_answer_text)
        if not numbers:
            return None  # Handle the case where no number is found
        last_number = float(numbers[-1])
        return int(last_number) if last_number.is_integer() else last_number

    def force_int(self, df):
        """Force integer conversion for numeric responses."""
        for idx, row in df.iterrows():
            if isinstance(row['response'], (float, int)) and float(row['response']).is_integer():
                df.at[idx, 'response'] = int(row['response'])
        return df

if __name__ == "__main__":
    df = filtered_df
    model_name = "llmware/bling-1b-0.1"
    model = FinancePredictor(model_name)
    outputs = model.make_predictions(filtered_df)
    results_df = pd.DataFrame(outputs)
    results_df['response'] = results_df['response'].apply(extract_final_answer)
    results_df = force_int(results_df)
    results_df.to_csv("predictions.csv", index=False)

